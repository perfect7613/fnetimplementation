{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNzzl93i4rXvGD2Y3Tj41qY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/perfect7613/fnetimplementation/blob/main/Fnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPr56vpysDJt",
        "outputId": "fae6cda3-a9bb-4d9a-e63a-0b8f5ec0c409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.12)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n",
            "Collecting pywavelets\n",
            "  Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from pywavelets) (1.26.4)\n",
            "Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pywavelets\n",
            "Successfully installed pywavelets-1.8.0\n",
            "Collecting torchtext\n",
            "  Downloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.32.3)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n",
            "Downloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchtext\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchtext-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric\n",
        "!pip install pywavelets\n",
        "!pip install torchtext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader as GeometricDataLoader\n",
        "import pywt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader as TorchDataLoader"
      ],
      "metadata": {
        "id": "loSC5JiXs-2i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LearnableWaveletLayer(nn.Module):\n",
        "    def __init__(self, input_dim, num_wavelets, wavelet_name='morl'):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim  # Now represents character signal dimension (e.g., 1 for scalar values)\n",
        "        self.num_wavelets = num_wavelets\n",
        "        self.wavelet_name = wavelet_name\n",
        "        self.scales = nn.Parameter(torch.rand(num_wavelets) + 1.0)\n",
        "        self.translations = nn.Parameter(torch.randn(num_wavelets))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        wavelet_coeffs = []\n",
        "        for i in range(self.num_wavelets):\n",
        "            coeffs_batch = []\n",
        "            for b in range(batch_size):\n",
        "                coeffs_embed = []\n",
        "                for e in range(self.input_dim):\n",
        "                    data = x[b, :, e].detach().cpu().numpy()\n",
        "                    scale = torch.abs(self.scales[i]).cpu().item() + 1e-6\n",
        "                    if scale < 1: scale = 1\n",
        "                    coeffs, freqs = pywt.cwt(data, scale, self.wavelet_name)\n",
        "                    coeffs_embed.append(torch.tensor(coeffs, dtype=torch.float32, device=x.device))\n",
        "                coeffs_batch.append(torch.stack(coeffs_embed))\n",
        "            wavelet_coeffs.append(torch.stack(coeffs_batch))\n",
        "        wavelet_coeffs = torch.stack(wavelet_coeffs).permute(1, 2, 0, 3, 4)\n",
        "        wavelet_coeffs = wavelet_coeffs.reshape(batch_size, seq_len, self.input_dim * self.num_wavelets)\n",
        "        return wavelet_coeffs\n",
        "\n",
        "    def inverse_transform(self, coeffs):\n",
        "        batch_size, seq_len, hidden_dim = coeffs.shape\n",
        "        reconstructed = torch.zeros(batch_size, seq_len, hidden_dim, device=coeffs.device)\n",
        "        temp_coeffs = torch.zeros(batch_size, seq_len, hidden_dim * self.num_wavelets, device=coeffs.device)\n",
        "        temp_coeffs[:, :, :hidden_dim] = coeffs\n",
        "        temp_coeffs = temp_coeffs.reshape(batch_size, hidden_dim, self.num_wavelets, seq_len).permute(2, 0, 1, 3)\n",
        "        for i in range(self.num_wavelets):\n",
        "            current_coeffs = temp_coeffs[i]\n",
        "            for b in range(batch_size):\n",
        "                for e in range(hidden_dim):\n",
        "                    data = current_coeffs[b, e].detach().cpu().numpy()\n",
        "                    rec_signal = pywt.waverec([data], self.wavelet_name, mode='per')\n",
        "                    rec_signal = np.array(rec_signal)\n",
        "                    if len(rec_signal) > seq_len: rec_signal = rec_signal[:seq_len]\n",
        "                    elif len(rec_signal) < seq_len: rec_signal = np.pad(rec_signal, (0, seq_len - len(rec_signal)))\n",
        "                    reconstructed[b, :, e] = torch.tensor(rec_signal, dtype=torch.float32, device=coeffs.device)\n",
        "        return reconstructed\n",
        "\n",
        "# --- Modified URM without Tokenization ---\n",
        "\n",
        "class URM(nn.Module):\n",
        "    def __init__(self, char_set_size, hidden_dim, num_layers, num_wavelets, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_wavelets = num_wavelets\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # No embedding layer; input_dim = 1 (character signal)\n",
        "        self.wavelet_layer = LearnableWaveletLayer(input_dim=1, num_wavelets=num_wavelets)\n",
        "\n",
        "        # Convolutional modulation: input channels = num_wavelets (since input_dim=1)\n",
        "        self.conv_mod = nn.Conv1d(num_wavelets, hidden_dim, kernel_size=3, padding=1)\n",
        "\n",
        "        # Graph Convolutional Network\n",
        "        self.gcn_layers = nn.ModuleList([GCNConv(hidden_dim, hidden_dim) for _ in range(num_layers)])\n",
        "        self.gcn_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Feedforward Network\n",
        "        self.ffn1 = nn.Linear(hidden_dim, hidden_dim * 4)\n",
        "        self.ffn2 = nn.Linear(hidden_dim * 4, hidden_dim)\n",
        "        self.ffn_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Output layer predicts characters\n",
        "        self.output_layer = nn.Linear(hidden_dim, char_set_size)\n",
        "\n",
        "    def forward(self, input_signal, dependency_graphs):\n",
        "        # input_signal: [batch_size, seq_len, 1] (raw character values)\n",
        "\n",
        "        # 1. Learnable Wavelet Transform\n",
        "        wavelet_coeffs = self.wavelet_layer(input_signal)  # [batch_size, seq_len, num_wavelets]\n",
        "\n",
        "        # 2. Convolutional Modulation\n",
        "        modulated_coeffs = self.conv_mod(wavelet_coeffs.transpose(1, 2)).transpose(1, 2)\n",
        "        modulated_coeffs = F.gelu(modulated_coeffs)  # [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        # 3. Inverse Wavelet Transform\n",
        "        reconstructed = self.wavelet_layer.inverse_transform(modulated_coeffs)\n",
        "\n",
        "        # 4. Prepare for GCN\n",
        "        graph_data = [Data(x=reconstructed[i], edge_index=adj_matrix.nonzero().t())\n",
        "                      for i, adj_matrix in enumerate(dependency_graphs)]\n",
        "        graph_loader = GeometricDataLoader(graph_data, batch_size=len(graph_data))\n",
        "        batch = next(iter(graph_loader))\n",
        "\n",
        "        # 5. Graph Convolutional Network\n",
        "        x = batch.x\n",
        "        edge_index = batch.edge_index\n",
        "        for gcn_layer in self.gcn_layers:\n",
        "            x = gcn_layer(x, edge_index)\n",
        "            x = F.gelu(x)\n",
        "            x = self.gcn_dropout(x)\n",
        "        x = x.reshape(len(graph_data), -1, self.hidden_dim)\n",
        "\n",
        "        # 6. Feedforward Network\n",
        "        x = F.gelu(self.ffn1(x))\n",
        "        x = self.ffn_dropout(x)\n",
        "        x = self.ffn2(x)\n",
        "\n",
        "        # 7. Output Layer (predict characters)\n",
        "        logits = self.output_layer(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "4i0y1dpduMOV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHZEIG_HuSTU",
        "outputId": "42a12d6b-6cf0-4968-e29a-cc66cfbda51f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-23 07:33:00--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-02-23 07:33:00 (27.4 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Create character set\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "char_set_size = len(chars)\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_dim = 64\n",
        "num_layers = 3\n",
        "num_wavelets = 4\n",
        "batch_size = 4\n",
        "seq_len = 10\n",
        "num_epochs = 600\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "wy2MumD2uVb_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_signal(text, seq_len, char_to_idx):\n",
        "    signal = [char_to_idx[ch] for ch in text]\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    for i in range(0, len(signal) - seq_len, seq_len):\n",
        "        seq = signal[i:i + seq_len]\n",
        "        label = signal[i + 1:i + seq_len + 1]  # Next-character prediction\n",
        "        sequences.append(seq)\n",
        "        labels.append(label)\n",
        "    return sequences, labels\n",
        "\n",
        "sequences, labels = text_to_signal(text, seq_len, char_to_idx)\n",
        "sequences = torch.tensor(sequences[:batch_size * 10], dtype=torch.float32).unsqueeze(-1)  # [N, seq_len, 1]\n",
        "labels = torch.tensor(labels[:batch_size * 10], dtype=torch.long)\n",
        "\n",
        "# Generate simple dependency graphs (linear chain)\n",
        "def get_dependency_graph(batch_size, seq_len):\n",
        "    dependency_graphs = []\n",
        "    for _ in range(batch_size):\n",
        "        adj_matrix = torch.zeros((seq_len, seq_len))\n",
        "        for i in range(seq_len - 1):\n",
        "            adj_matrix[i, i + 1] = 1\n",
        "            adj_matrix[i + 1, i] = 1\n",
        "        dependency_graphs.append(adj_matrix)\n",
        "    return dependency_graphs\n",
        "\n",
        "dependency_graphs_list = get_dependency_graph(len(sequences), seq_len)"
      ],
      "metadata": {
        "id": "ZToVyOrKua8e"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, input_signal, dependency_graphs, labels):\n",
        "        self.input_signal = input_signal\n",
        "        self.dependency_graphs = dependency_graphs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_signal)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_signal': self.input_signal[idx],\n",
        "            'dependency_graph': self.dependency_graphs[idx],\n",
        "            'label': self.labels[idx]\n",
        "        }\n",
        "\n",
        "dataset = TextDataset(sequences, dependency_graphs_list, labels)\n",
        "dataloader = TorchDataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# --- Model, Loss, and Optimizer ---\n",
        "\n",
        "model = URM(char_set_size, hidden_dim, num_layers, num_wavelets)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# --- Training Loop ---\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        input_signal_batch = batch['input_signal']\n",
        "        dependency_graphs_batch = batch['dependency_graph']\n",
        "        labels_batch = batch['label']\n",
        "        logits = model(input_signal_batch, dependency_graphs_batch)\n",
        "        loss = criterion(logits.view(-1, char_set_size), labels_batch.view(-1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Training Complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeTCfzS1ugbt",
        "outputId": "88d3229f-439b-470c-e2b4-193aa0fbb3f9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/600, Loss: 4.0000\n",
            "Epoch 2/600, Loss: 3.5341\n",
            "Epoch 3/600, Loss: 3.3350\n",
            "Epoch 4/600, Loss: 3.2742\n",
            "Epoch 5/600, Loss: 3.2174\n",
            "Epoch 6/600, Loss: 3.1910\n",
            "Epoch 7/600, Loss: 3.1621\n",
            "Epoch 8/600, Loss: 3.1312\n",
            "Epoch 9/600, Loss: 3.1063\n",
            "Epoch 10/600, Loss: 3.0826\n",
            "Epoch 11/600, Loss: 3.0507\n",
            "Epoch 12/600, Loss: 3.0138\n",
            "Epoch 13/600, Loss: 2.9796\n",
            "Epoch 14/600, Loss: 2.9516\n",
            "Epoch 15/600, Loss: 2.9508\n",
            "Epoch 16/600, Loss: 2.9288\n",
            "Epoch 17/600, Loss: 2.8720\n",
            "Epoch 18/600, Loss: 2.8248\n",
            "Epoch 19/600, Loss: 2.8017\n",
            "Epoch 20/600, Loss: 2.7901\n",
            "Epoch 21/600, Loss: 2.7767\n",
            "Epoch 22/600, Loss: 2.7615\n",
            "Epoch 23/600, Loss: 2.7226\n",
            "Epoch 24/600, Loss: 2.7257\n",
            "Epoch 25/600, Loss: 2.6756\n",
            "Epoch 26/600, Loss: 2.6058\n",
            "Epoch 27/600, Loss: 2.5834\n",
            "Epoch 28/600, Loss: 2.5717\n",
            "Epoch 29/600, Loss: 2.5462\n",
            "Epoch 30/600, Loss: 2.5245\n",
            "Epoch 31/600, Loss: 2.4781\n",
            "Epoch 32/600, Loss: 2.4439\n",
            "Epoch 33/600, Loss: 2.4781\n",
            "Epoch 34/600, Loss: 2.4350\n",
            "Epoch 35/600, Loss: 2.3962\n",
            "Epoch 36/600, Loss: 2.3750\n",
            "Epoch 37/600, Loss: 2.3329\n",
            "Epoch 38/600, Loss: 2.3261\n",
            "Epoch 39/600, Loss: 2.2423\n",
            "Epoch 40/600, Loss: 2.2702\n",
            "Epoch 41/600, Loss: 2.2811\n",
            "Epoch 42/600, Loss: 2.2146\n",
            "Epoch 43/600, Loss: 2.2149\n",
            "Epoch 44/600, Loss: 2.1629\n",
            "Epoch 45/600, Loss: 2.1727\n",
            "Epoch 46/600, Loss: 2.1458\n",
            "Epoch 47/600, Loss: 2.1175\n",
            "Epoch 48/600, Loss: 2.0882\n",
            "Epoch 49/600, Loss: 2.0844\n",
            "Epoch 50/600, Loss: 1.9888\n",
            "Epoch 51/600, Loss: 2.0345\n",
            "Epoch 52/600, Loss: 2.0844\n",
            "Epoch 53/600, Loss: 1.9850\n",
            "Epoch 54/600, Loss: 1.9539\n",
            "Epoch 55/600, Loss: 1.9183\n",
            "Epoch 56/600, Loss: 1.9270\n",
            "Epoch 57/600, Loss: 1.9259\n",
            "Epoch 58/600, Loss: 1.9151\n",
            "Epoch 59/600, Loss: 1.9209\n",
            "Epoch 60/600, Loss: 1.8248\n",
            "Epoch 61/600, Loss: 1.8160\n",
            "Epoch 62/600, Loss: 1.7792\n",
            "Epoch 63/600, Loss: 1.7683\n",
            "Epoch 64/600, Loss: 1.7627\n",
            "Epoch 65/600, Loss: 1.7356\n",
            "Epoch 66/600, Loss: 1.7912\n",
            "Epoch 67/600, Loss: 1.6529\n",
            "Epoch 68/600, Loss: 1.7382\n",
            "Epoch 69/600, Loss: 1.6910\n",
            "Epoch 70/600, Loss: 1.5819\n",
            "Epoch 71/600, Loss: 1.6272\n",
            "Epoch 72/600, Loss: 1.5886\n",
            "Epoch 73/600, Loss: 1.5982\n",
            "Epoch 74/600, Loss: 1.5712\n",
            "Epoch 75/600, Loss: 1.5659\n",
            "Epoch 76/600, Loss: 1.5965\n",
            "Epoch 77/600, Loss: 1.5467\n",
            "Epoch 78/600, Loss: 1.5014\n",
            "Epoch 79/600, Loss: 1.5050\n",
            "Epoch 80/600, Loss: 1.4704\n",
            "Epoch 81/600, Loss: 1.4280\n",
            "Epoch 82/600, Loss: 1.4336\n",
            "Epoch 83/600, Loss: 1.3421\n",
            "Epoch 84/600, Loss: 1.3781\n",
            "Epoch 85/600, Loss: 1.4049\n",
            "Epoch 86/600, Loss: 1.4912\n",
            "Epoch 87/600, Loss: 1.3699\n",
            "Epoch 88/600, Loss: 1.2769\n",
            "Epoch 89/600, Loss: 1.3299\n",
            "Epoch 90/600, Loss: 1.3145\n",
            "Epoch 91/600, Loss: 1.3043\n",
            "Epoch 92/600, Loss: 1.3084\n",
            "Epoch 93/600, Loss: 1.3777\n",
            "Epoch 94/600, Loss: 1.3014\n",
            "Epoch 95/600, Loss: 1.3220\n",
            "Epoch 96/600, Loss: 1.2770\n",
            "Epoch 97/600, Loss: 1.2867\n",
            "Epoch 98/600, Loss: 1.1964\n",
            "Epoch 99/600, Loss: 1.2464\n",
            "Epoch 100/600, Loss: 1.2293\n",
            "Epoch 101/600, Loss: 1.1704\n",
            "Epoch 102/600, Loss: 1.1630\n",
            "Epoch 103/600, Loss: 1.1620\n",
            "Epoch 104/600, Loss: 1.0827\n",
            "Epoch 105/600, Loss: 1.2308\n",
            "Epoch 106/600, Loss: 1.1024\n",
            "Epoch 107/600, Loss: 1.1405\n",
            "Epoch 108/600, Loss: 1.1348\n",
            "Epoch 109/600, Loss: 1.1920\n",
            "Epoch 110/600, Loss: 1.1637\n",
            "Epoch 111/600, Loss: 1.1351\n",
            "Epoch 112/600, Loss: 1.0574\n",
            "Epoch 113/600, Loss: 1.0930\n",
            "Epoch 114/600, Loss: 1.0715\n",
            "Epoch 115/600, Loss: 1.0411\n",
            "Epoch 116/600, Loss: 1.0195\n",
            "Epoch 117/600, Loss: 1.0718\n",
            "Epoch 118/600, Loss: 1.0141\n",
            "Epoch 119/600, Loss: 1.0339\n",
            "Epoch 120/600, Loss: 1.0507\n",
            "Epoch 121/600, Loss: 1.0250\n",
            "Epoch 122/600, Loss: 0.9690\n",
            "Epoch 123/600, Loss: 1.0259\n",
            "Epoch 124/600, Loss: 1.0227\n",
            "Epoch 125/600, Loss: 0.9226\n",
            "Epoch 126/600, Loss: 0.9989\n",
            "Epoch 127/600, Loss: 0.9654\n",
            "Epoch 128/600, Loss: 0.9092\n",
            "Epoch 129/600, Loss: 0.8979\n",
            "Epoch 130/600, Loss: 0.9170\n",
            "Epoch 131/600, Loss: 0.8877\n",
            "Epoch 132/600, Loss: 0.9511\n",
            "Epoch 133/600, Loss: 0.9166\n",
            "Epoch 134/600, Loss: 0.8960\n",
            "Epoch 135/600, Loss: 0.8731\n",
            "Epoch 136/600, Loss: 0.8505\n",
            "Epoch 137/600, Loss: 0.9193\n",
            "Epoch 138/600, Loss: 0.8123\n",
            "Epoch 139/600, Loss: 0.9322\n",
            "Epoch 140/600, Loss: 0.8601\n",
            "Epoch 141/600, Loss: 0.8728\n",
            "Epoch 142/600, Loss: 0.8359\n",
            "Epoch 143/600, Loss: 0.8077\n",
            "Epoch 144/600, Loss: 0.8611\n",
            "Epoch 145/600, Loss: 0.7784\n",
            "Epoch 146/600, Loss: 0.8029\n",
            "Epoch 147/600, Loss: 0.7354\n",
            "Epoch 148/600, Loss: 0.8303\n",
            "Epoch 149/600, Loss: 0.7437\n",
            "Epoch 150/600, Loss: 0.7615\n",
            "Epoch 151/600, Loss: 0.8105\n",
            "Epoch 152/600, Loss: 0.8222\n",
            "Epoch 153/600, Loss: 0.7332\n",
            "Epoch 154/600, Loss: 0.7688\n",
            "Epoch 155/600, Loss: 0.7584\n",
            "Epoch 156/600, Loss: 0.7463\n",
            "Epoch 157/600, Loss: 0.7567\n",
            "Epoch 158/600, Loss: 0.7195\n",
            "Epoch 159/600, Loss: 0.7674\n",
            "Epoch 160/600, Loss: 0.7581\n",
            "Epoch 161/600, Loss: 0.7399\n",
            "Epoch 162/600, Loss: 0.7420\n",
            "Epoch 163/600, Loss: 0.7610\n",
            "Epoch 164/600, Loss: 0.6488\n",
            "Epoch 165/600, Loss: 0.6554\n",
            "Epoch 166/600, Loss: 0.6669\n",
            "Epoch 167/600, Loss: 0.7182\n",
            "Epoch 168/600, Loss: 0.7888\n",
            "Epoch 169/600, Loss: 0.7738\n",
            "Epoch 170/600, Loss: 0.7514\n",
            "Epoch 171/600, Loss: 0.7223\n",
            "Epoch 172/600, Loss: 0.7678\n",
            "Epoch 173/600, Loss: 0.7147\n",
            "Epoch 174/600, Loss: 0.7662\n",
            "Epoch 175/600, Loss: 0.7589\n",
            "Epoch 176/600, Loss: 0.6595\n",
            "Epoch 177/600, Loss: 0.5827\n",
            "Epoch 178/600, Loss: 0.7016\n",
            "Epoch 179/600, Loss: 0.6462\n",
            "Epoch 180/600, Loss: 0.6933\n",
            "Epoch 181/600, Loss: 0.6220\n",
            "Epoch 182/600, Loss: 0.6107\n",
            "Epoch 183/600, Loss: 0.6195\n",
            "Epoch 184/600, Loss: 0.6124\n",
            "Epoch 185/600, Loss: 0.6307\n",
            "Epoch 186/600, Loss: 0.5862\n",
            "Epoch 187/600, Loss: 0.6807\n",
            "Epoch 188/600, Loss: 0.6613\n",
            "Epoch 189/600, Loss: 0.6590\n",
            "Epoch 190/600, Loss: 0.6099\n",
            "Epoch 191/600, Loss: 0.5925\n",
            "Epoch 192/600, Loss: 0.6588\n",
            "Epoch 193/600, Loss: 0.6135\n",
            "Epoch 194/600, Loss: 0.6077\n",
            "Epoch 195/600, Loss: 0.5890\n",
            "Epoch 196/600, Loss: 0.5427\n",
            "Epoch 197/600, Loss: 0.5978\n",
            "Epoch 198/600, Loss: 0.6369\n",
            "Epoch 199/600, Loss: 0.5546\n",
            "Epoch 200/600, Loss: 0.5071\n",
            "Epoch 201/600, Loss: 0.5916\n",
            "Epoch 202/600, Loss: 0.5519\n",
            "Epoch 203/600, Loss: 0.5951\n",
            "Epoch 204/600, Loss: 0.6219\n",
            "Epoch 205/600, Loss: 0.5368\n",
            "Epoch 206/600, Loss: 0.5631\n",
            "Epoch 207/600, Loss: 0.5251\n",
            "Epoch 208/600, Loss: 0.5127\n",
            "Epoch 209/600, Loss: 0.5165\n",
            "Epoch 210/600, Loss: 0.4931\n",
            "Epoch 211/600, Loss: 0.5598\n",
            "Epoch 212/600, Loss: 0.5867\n",
            "Epoch 213/600, Loss: 0.5589\n",
            "Epoch 214/600, Loss: 0.5647\n",
            "Epoch 215/600, Loss: 0.5320\n",
            "Epoch 216/600, Loss: 0.5398\n",
            "Epoch 217/600, Loss: 0.5178\n",
            "Epoch 218/600, Loss: 0.5309\n",
            "Epoch 219/600, Loss: 0.5532\n",
            "Epoch 220/600, Loss: 0.4869\n",
            "Epoch 221/600, Loss: 0.5744\n",
            "Epoch 222/600, Loss: 0.5251\n",
            "Epoch 223/600, Loss: 0.5021\n",
            "Epoch 224/600, Loss: 0.5132\n",
            "Epoch 225/600, Loss: 0.5073\n",
            "Epoch 226/600, Loss: 0.5585\n",
            "Epoch 227/600, Loss: 0.5798\n",
            "Epoch 228/600, Loss: 0.5318\n",
            "Epoch 229/600, Loss: 0.5156\n",
            "Epoch 230/600, Loss: 0.5228\n",
            "Epoch 231/600, Loss: 0.4481\n",
            "Epoch 232/600, Loss: 0.5303\n",
            "Epoch 233/600, Loss: 0.5126\n",
            "Epoch 234/600, Loss: 0.5115\n",
            "Epoch 235/600, Loss: 0.4404\n",
            "Epoch 236/600, Loss: 0.4159\n",
            "Epoch 237/600, Loss: 0.4665\n",
            "Epoch 238/600, Loss: 0.4536\n",
            "Epoch 239/600, Loss: 0.3951\n",
            "Epoch 240/600, Loss: 0.4777\n",
            "Epoch 241/600, Loss: 0.4832\n",
            "Epoch 242/600, Loss: 0.5232\n",
            "Epoch 243/600, Loss: 0.4509\n",
            "Epoch 244/600, Loss: 0.5043\n",
            "Epoch 245/600, Loss: 0.4446\n",
            "Epoch 246/600, Loss: 0.5568\n",
            "Epoch 247/600, Loss: 0.4524\n",
            "Epoch 248/600, Loss: 0.4673\n",
            "Epoch 249/600, Loss: 0.4507\n",
            "Epoch 250/600, Loss: 0.4488\n",
            "Epoch 251/600, Loss: 0.4365\n",
            "Epoch 252/600, Loss: 0.5204\n",
            "Epoch 253/600, Loss: 0.5231\n",
            "Epoch 254/600, Loss: 0.3970\n",
            "Epoch 255/600, Loss: 0.4809\n",
            "Epoch 256/600, Loss: 0.4945\n",
            "Epoch 257/600, Loss: 0.4613\n",
            "Epoch 258/600, Loss: 0.4599\n",
            "Epoch 259/600, Loss: 0.4340\n",
            "Epoch 260/600, Loss: 0.4752\n",
            "Epoch 261/600, Loss: 0.4344\n",
            "Epoch 262/600, Loss: 0.4252\n",
            "Epoch 263/600, Loss: 0.4779\n",
            "Epoch 264/600, Loss: 0.5267\n",
            "Epoch 265/600, Loss: 0.4615\n",
            "Epoch 266/600, Loss: 0.4271\n",
            "Epoch 267/600, Loss: 0.4303\n",
            "Epoch 268/600, Loss: 0.3736\n",
            "Epoch 269/600, Loss: 0.3473\n",
            "Epoch 270/600, Loss: 0.3725\n",
            "Epoch 271/600, Loss: 0.4388\n",
            "Epoch 272/600, Loss: 0.3972\n",
            "Epoch 273/600, Loss: 0.4548\n",
            "Epoch 274/600, Loss: 0.4257\n",
            "Epoch 275/600, Loss: 0.3809\n",
            "Epoch 276/600, Loss: 0.3802\n",
            "Epoch 277/600, Loss: 0.4731\n",
            "Epoch 278/600, Loss: 0.4379\n",
            "Epoch 279/600, Loss: 0.4001\n",
            "Epoch 280/600, Loss: 0.3695\n",
            "Epoch 281/600, Loss: 0.3486\n",
            "Epoch 282/600, Loss: 0.3815\n",
            "Epoch 283/600, Loss: 0.4053\n",
            "Epoch 284/600, Loss: 0.4256\n",
            "Epoch 285/600, Loss: 0.4009\n",
            "Epoch 286/600, Loss: 0.3681\n",
            "Epoch 287/600, Loss: 0.3713\n",
            "Epoch 288/600, Loss: 0.3989\n",
            "Epoch 289/600, Loss: 0.4528\n",
            "Epoch 290/600, Loss: 0.4300\n",
            "Epoch 291/600, Loss: 0.4276\n",
            "Epoch 292/600, Loss: 0.3230\n",
            "Epoch 293/600, Loss: 0.3771\n",
            "Epoch 294/600, Loss: 0.3839\n",
            "Epoch 295/600, Loss: 0.3987\n",
            "Epoch 296/600, Loss: 0.4693\n",
            "Epoch 297/600, Loss: 0.4347\n",
            "Epoch 298/600, Loss: 0.3591\n",
            "Epoch 299/600, Loss: 0.3915\n",
            "Epoch 300/600, Loss: 0.3334\n",
            "Epoch 301/600, Loss: 0.3265\n",
            "Epoch 302/600, Loss: 0.3462\n",
            "Epoch 303/600, Loss: 0.3580\n",
            "Epoch 304/600, Loss: 0.4260\n",
            "Epoch 305/600, Loss: 0.4583\n",
            "Epoch 306/600, Loss: 0.3710\n",
            "Epoch 307/600, Loss: 0.4027\n",
            "Epoch 308/600, Loss: 0.3738\n",
            "Epoch 309/600, Loss: 0.3607\n",
            "Epoch 310/600, Loss: 0.4007\n",
            "Epoch 311/600, Loss: 0.3997\n",
            "Epoch 312/600, Loss: 0.3288\n",
            "Epoch 313/600, Loss: 0.3804\n",
            "Epoch 314/600, Loss: 0.3109\n",
            "Epoch 315/600, Loss: 0.3247\n",
            "Epoch 316/600, Loss: 0.3799\n",
            "Epoch 317/600, Loss: 0.3993\n",
            "Epoch 318/600, Loss: 0.3400\n",
            "Epoch 319/600, Loss: 0.3734\n",
            "Epoch 320/600, Loss: 0.2975\n",
            "Epoch 321/600, Loss: 0.3677\n",
            "Epoch 322/600, Loss: 0.4046\n",
            "Epoch 323/600, Loss: 0.2937\n",
            "Epoch 324/600, Loss: 0.3628\n",
            "Epoch 325/600, Loss: 0.3852\n",
            "Epoch 326/600, Loss: 0.3434\n",
            "Epoch 327/600, Loss: 0.3373\n",
            "Epoch 328/600, Loss: 0.3120\n",
            "Epoch 329/600, Loss: 0.3508\n",
            "Epoch 330/600, Loss: 0.3767\n",
            "Epoch 331/600, Loss: 0.3234\n",
            "Epoch 332/600, Loss: 0.2898\n",
            "Epoch 333/600, Loss: 0.3087\n",
            "Epoch 334/600, Loss: 0.3357\n",
            "Epoch 335/600, Loss: 0.3696\n",
            "Epoch 336/600, Loss: 0.3121\n",
            "Epoch 337/600, Loss: 0.2960\n",
            "Epoch 338/600, Loss: 0.3856\n",
            "Epoch 339/600, Loss: 0.4647\n",
            "Epoch 340/600, Loss: 0.3672\n",
            "Epoch 341/600, Loss: 0.3352\n",
            "Epoch 342/600, Loss: 0.3995\n",
            "Epoch 343/600, Loss: 0.3323\n",
            "Epoch 344/600, Loss: 0.3739\n",
            "Epoch 345/600, Loss: 0.3632\n",
            "Epoch 346/600, Loss: 0.3160\n",
            "Epoch 347/600, Loss: 0.2889\n",
            "Epoch 348/600, Loss: 0.3265\n",
            "Epoch 349/600, Loss: 0.3507\n",
            "Epoch 350/600, Loss: 0.3092\n",
            "Epoch 351/600, Loss: 0.2769\n",
            "Epoch 352/600, Loss: 0.2797\n",
            "Epoch 353/600, Loss: 0.3389\n",
            "Epoch 354/600, Loss: 0.3339\n",
            "Epoch 355/600, Loss: 0.3564\n",
            "Epoch 356/600, Loss: 0.3310\n",
            "Epoch 357/600, Loss: 0.2887\n",
            "Epoch 358/600, Loss: 0.2631\n",
            "Epoch 359/600, Loss: 0.2989\n",
            "Epoch 360/600, Loss: 0.3541\n",
            "Epoch 361/600, Loss: 0.3432\n",
            "Epoch 362/600, Loss: 0.3788\n",
            "Epoch 363/600, Loss: 0.3500\n",
            "Epoch 364/600, Loss: 0.3360\n",
            "Epoch 365/600, Loss: 0.2774\n",
            "Epoch 366/600, Loss: 0.3505\n",
            "Epoch 367/600, Loss: 0.2671\n",
            "Epoch 368/600, Loss: 0.3066\n",
            "Epoch 369/600, Loss: 0.3403\n",
            "Epoch 370/600, Loss: 0.3823\n",
            "Epoch 371/600, Loss: 0.3591\n",
            "Epoch 372/600, Loss: 0.2720\n",
            "Epoch 373/600, Loss: 0.3548\n",
            "Epoch 374/600, Loss: 0.3724\n",
            "Epoch 375/600, Loss: 0.2831\n",
            "Epoch 376/600, Loss: 0.3469\n",
            "Epoch 377/600, Loss: 0.2836\n",
            "Epoch 378/600, Loss: 0.3109\n",
            "Epoch 379/600, Loss: 0.3768\n",
            "Epoch 380/600, Loss: 0.2799\n",
            "Epoch 381/600, Loss: 0.2865\n",
            "Epoch 382/600, Loss: 0.3037\n",
            "Epoch 383/600, Loss: 0.2967\n",
            "Epoch 384/600, Loss: 0.3331\n",
            "Epoch 385/600, Loss: 0.2852\n",
            "Epoch 386/600, Loss: 0.3120\n",
            "Epoch 387/600, Loss: 0.3067\n",
            "Epoch 388/600, Loss: 0.2602\n",
            "Epoch 389/600, Loss: 0.2997\n",
            "Epoch 390/600, Loss: 0.2412\n",
            "Epoch 391/600, Loss: 0.3325\n",
            "Epoch 392/600, Loss: 0.2609\n",
            "Epoch 393/600, Loss: 0.2445\n",
            "Epoch 394/600, Loss: 0.2949\n",
            "Epoch 395/600, Loss: 0.3494\n",
            "Epoch 396/600, Loss: 0.3519\n",
            "Epoch 397/600, Loss: 0.3022\n",
            "Epoch 398/600, Loss: 0.3879\n",
            "Epoch 399/600, Loss: 0.3479\n",
            "Epoch 400/600, Loss: 0.3192\n",
            "Epoch 401/600, Loss: 0.3200\n",
            "Epoch 402/600, Loss: 0.3533\n",
            "Epoch 403/600, Loss: 0.3427\n",
            "Epoch 404/600, Loss: 0.3431\n",
            "Epoch 405/600, Loss: 0.3312\n",
            "Epoch 406/600, Loss: 0.2699\n",
            "Epoch 407/600, Loss: 0.2874\n",
            "Epoch 408/600, Loss: 0.2694\n",
            "Epoch 409/600, Loss: 0.3189\n",
            "Epoch 410/600, Loss: 0.3192\n",
            "Epoch 411/600, Loss: 0.2583\n",
            "Epoch 412/600, Loss: 0.2620\n",
            "Epoch 413/600, Loss: 0.2355\n",
            "Epoch 414/600, Loss: 0.3029\n",
            "Epoch 415/600, Loss: 0.2799\n",
            "Epoch 416/600, Loss: 0.3275\n",
            "Epoch 417/600, Loss: 0.3051\n",
            "Epoch 418/600, Loss: 0.2571\n",
            "Epoch 419/600, Loss: 0.2859\n",
            "Epoch 420/600, Loss: 0.3187\n",
            "Epoch 421/600, Loss: 0.3542\n",
            "Epoch 422/600, Loss: 0.3110\n",
            "Epoch 423/600, Loss: 0.2760\n",
            "Epoch 424/600, Loss: 0.2210\n",
            "Epoch 425/600, Loss: 0.2887\n",
            "Epoch 426/600, Loss: 0.2616\n",
            "Epoch 427/600, Loss: 0.2953\n",
            "Epoch 428/600, Loss: 0.2647\n",
            "Epoch 429/600, Loss: 0.2497\n",
            "Epoch 430/600, Loss: 0.2655\n",
            "Epoch 431/600, Loss: 0.2852\n",
            "Epoch 432/600, Loss: 0.2575\n",
            "Epoch 433/600, Loss: 0.2380\n",
            "Epoch 434/600, Loss: 0.2740\n",
            "Epoch 435/600, Loss: 0.2594\n",
            "Epoch 436/600, Loss: 0.2871\n",
            "Epoch 437/600, Loss: 0.2811\n",
            "Epoch 438/600, Loss: 0.2374\n",
            "Epoch 439/600, Loss: 0.2598\n",
            "Epoch 440/600, Loss: 0.2246\n",
            "Epoch 441/600, Loss: 0.2559\n",
            "Epoch 442/600, Loss: 0.2035\n",
            "Epoch 443/600, Loss: 0.2447\n",
            "Epoch 444/600, Loss: 0.2433\n",
            "Epoch 445/600, Loss: 0.2684\n",
            "Epoch 446/600, Loss: 0.2520\n",
            "Epoch 447/600, Loss: 0.2506\n",
            "Epoch 448/600, Loss: 0.2449\n",
            "Epoch 449/600, Loss: 0.2979\n",
            "Epoch 450/600, Loss: 0.2922\n",
            "Epoch 451/600, Loss: 0.3086\n",
            "Epoch 452/600, Loss: 0.2398\n",
            "Epoch 453/600, Loss: 0.2726\n",
            "Epoch 454/600, Loss: 0.2469\n",
            "Epoch 455/600, Loss: 0.2430\n",
            "Epoch 456/600, Loss: 0.2911\n",
            "Epoch 457/600, Loss: 0.2030\n",
            "Epoch 458/600, Loss: 0.2209\n",
            "Epoch 459/600, Loss: 0.2831\n",
            "Epoch 460/600, Loss: 0.2318\n",
            "Epoch 461/600, Loss: 0.2889\n",
            "Epoch 462/600, Loss: 0.2357\n",
            "Epoch 463/600, Loss: 0.2586\n",
            "Epoch 464/600, Loss: 0.2466\n",
            "Epoch 465/600, Loss: 0.2315\n",
            "Epoch 466/600, Loss: 0.2657\n",
            "Epoch 467/600, Loss: 0.2424\n",
            "Epoch 468/600, Loss: 0.1965\n",
            "Epoch 469/600, Loss: 0.3184\n",
            "Epoch 470/600, Loss: 0.2914\n",
            "Epoch 471/600, Loss: 0.2892\n",
            "Epoch 472/600, Loss: 0.2598\n",
            "Epoch 473/600, Loss: 0.1946\n",
            "Epoch 474/600, Loss: 0.2296\n",
            "Epoch 475/600, Loss: 0.2108\n",
            "Epoch 476/600, Loss: 0.2196\n",
            "Epoch 477/600, Loss: 0.2573\n",
            "Epoch 478/600, Loss: 0.3331\n",
            "Epoch 479/600, Loss: 0.2900\n",
            "Epoch 480/600, Loss: 0.3297\n",
            "Epoch 481/600, Loss: 0.2201\n",
            "Epoch 482/600, Loss: 0.2703\n",
            "Epoch 483/600, Loss: 0.2074\n",
            "Epoch 484/600, Loss: 0.2329\n",
            "Epoch 485/600, Loss: 0.1786\n",
            "Epoch 486/600, Loss: 0.1656\n",
            "Epoch 487/600, Loss: 0.1883\n",
            "Epoch 488/600, Loss: 0.2033\n",
            "Epoch 489/600, Loss: 0.2360\n",
            "Epoch 490/600, Loss: 0.2278\n",
            "Epoch 491/600, Loss: 0.2281\n",
            "Epoch 492/600, Loss: 0.2556\n",
            "Epoch 493/600, Loss: 0.2553\n",
            "Epoch 494/600, Loss: 0.2436\n",
            "Epoch 495/600, Loss: 0.2855\n",
            "Epoch 496/600, Loss: 0.2975\n",
            "Epoch 497/600, Loss: 0.2360\n",
            "Epoch 498/600, Loss: 0.2334\n",
            "Epoch 499/600, Loss: 0.2579\n",
            "Epoch 500/600, Loss: 0.2184\n",
            "Epoch 501/600, Loss: 0.2794\n",
            "Epoch 502/600, Loss: 0.2724\n",
            "Epoch 503/600, Loss: 0.3126\n",
            "Epoch 504/600, Loss: 0.2197\n",
            "Epoch 505/600, Loss: 0.2274\n",
            "Epoch 506/600, Loss: 0.2469\n",
            "Epoch 507/600, Loss: 0.1945\n",
            "Epoch 508/600, Loss: 0.2157\n",
            "Epoch 509/600, Loss: 0.2080\n",
            "Epoch 510/600, Loss: 0.1991\n",
            "Epoch 511/600, Loss: 0.2418\n",
            "Epoch 512/600, Loss: 0.2183\n",
            "Epoch 513/600, Loss: 0.2891\n",
            "Epoch 514/600, Loss: 0.2592\n",
            "Epoch 515/600, Loss: 0.2645\n",
            "Epoch 516/600, Loss: 0.1885\n",
            "Epoch 517/600, Loss: 0.2543\n",
            "Epoch 518/600, Loss: 0.2377\n",
            "Epoch 519/600, Loss: 0.2294\n",
            "Epoch 520/600, Loss: 0.2379\n",
            "Epoch 521/600, Loss: 0.1971\n",
            "Epoch 522/600, Loss: 0.2082\n",
            "Epoch 523/600, Loss: 0.2462\n",
            "Epoch 524/600, Loss: 0.2526\n",
            "Epoch 525/600, Loss: 0.1962\n",
            "Epoch 526/600, Loss: 0.2940\n",
            "Epoch 527/600, Loss: 0.2037\n",
            "Epoch 528/600, Loss: 0.2268\n",
            "Epoch 529/600, Loss: 0.3208\n",
            "Epoch 530/600, Loss: 0.2251\n",
            "Epoch 531/600, Loss: 0.2489\n",
            "Epoch 532/600, Loss: 0.2596\n",
            "Epoch 533/600, Loss: 0.2671\n",
            "Epoch 534/600, Loss: 0.2613\n",
            "Epoch 535/600, Loss: 0.2151\n",
            "Epoch 536/600, Loss: 0.2874\n",
            "Epoch 537/600, Loss: 0.2411\n",
            "Epoch 538/600, Loss: 0.2411\n",
            "Epoch 539/600, Loss: 0.2179\n",
            "Epoch 540/600, Loss: 0.2142\n",
            "Epoch 541/600, Loss: 0.2709\n",
            "Epoch 542/600, Loss: 0.2284\n",
            "Epoch 543/600, Loss: 0.2052\n",
            "Epoch 544/600, Loss: 0.1906\n",
            "Epoch 545/600, Loss: 0.1883\n",
            "Epoch 546/600, Loss: 0.1943\n",
            "Epoch 547/600, Loss: 0.2373\n",
            "Epoch 548/600, Loss: 0.1620\n",
            "Epoch 549/600, Loss: 0.2077\n",
            "Epoch 550/600, Loss: 0.1806\n",
            "Epoch 551/600, Loss: 0.2070\n",
            "Epoch 552/600, Loss: 0.1936\n",
            "Epoch 553/600, Loss: 0.2167\n",
            "Epoch 554/600, Loss: 0.1941\n",
            "Epoch 555/600, Loss: 0.1741\n",
            "Epoch 556/600, Loss: 0.2638\n",
            "Epoch 557/600, Loss: 0.2458\n",
            "Epoch 558/600, Loss: 0.1775\n",
            "Epoch 559/600, Loss: 0.2308\n",
            "Epoch 560/600, Loss: 0.2294\n",
            "Epoch 561/600, Loss: 0.2918\n",
            "Epoch 562/600, Loss: 0.2335\n",
            "Epoch 563/600, Loss: 0.2284\n",
            "Epoch 564/600, Loss: 0.1913\n",
            "Epoch 565/600, Loss: 0.1757\n",
            "Epoch 566/600, Loss: 0.2100\n",
            "Epoch 567/600, Loss: 0.2080\n",
            "Epoch 568/600, Loss: 0.2149\n",
            "Epoch 569/600, Loss: 0.2431\n",
            "Epoch 570/600, Loss: 0.3085\n",
            "Epoch 571/600, Loss: 0.2414\n",
            "Epoch 572/600, Loss: 0.2094\n",
            "Epoch 573/600, Loss: 0.1726\n",
            "Epoch 574/600, Loss: 0.1956\n",
            "Epoch 575/600, Loss: 0.2284\n",
            "Epoch 576/600, Loss: 0.1838\n",
            "Epoch 577/600, Loss: 0.2176\n",
            "Epoch 578/600, Loss: 0.1958\n",
            "Epoch 579/600, Loss: 0.1987\n",
            "Epoch 580/600, Loss: 0.1743\n",
            "Epoch 581/600, Loss: 0.2321\n",
            "Epoch 582/600, Loss: 0.1805\n",
            "Epoch 583/600, Loss: 0.1951\n",
            "Epoch 584/600, Loss: 0.1807\n",
            "Epoch 585/600, Loss: 0.2076\n",
            "Epoch 586/600, Loss: 0.1440\n",
            "Epoch 587/600, Loss: 0.2284\n",
            "Epoch 588/600, Loss: 0.1863\n",
            "Epoch 589/600, Loss: 0.1808\n",
            "Epoch 590/600, Loss: 0.1501\n",
            "Epoch 591/600, Loss: 0.2009\n",
            "Epoch 592/600, Loss: 0.1669\n",
            "Epoch 593/600, Loss: 0.1814\n",
            "Epoch 594/600, Loss: 0.2059\n",
            "Epoch 595/600, Loss: 0.1698\n",
            "Epoch 596/600, Loss: 0.2037\n",
            "Epoch 597/600, Loss: 0.2253\n",
            "Epoch 598/600, Loss: 0.2484\n",
            "Epoch 599/600, Loss: 0.2573\n",
            "Epoch 600/600, Loss: 0.1946\n",
            "Training Complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    sample_input = sequences[:1]  # [1, seq_len, 1]\n",
        "    sample_graph = dependency_graphs_list[:1]\n",
        "    logits = model(sample_input, sample_graph)\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    predicted_chars = [idx_to_char[idx.item()] for idx in predictions[0]]\n",
        "    input_chars = [idx_to_char[int(idx.item())] for idx in sample_input[0, :, 0]]\n",
        "    print(\"\\nSample Input Characters:\", ''.join(input_chars))\n",
        "    print(\"Predicted Next Characters:\", ''.join(predicted_chars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4fPConJu1yO",
        "outputId": "36579fd8-541b-4d27-b300-34a88de18245"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample Input Characters: First Citi\n",
            "Predicted Next Characters: irst Citiz\n"
          ]
        }
      ]
    }
  ]
}